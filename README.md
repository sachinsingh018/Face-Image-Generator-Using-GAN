# Face_Generator

A GAN (Generative Adversarial Network) is a type of machine learning system developed by Ian Goodfellow and his team in 2014. It consists of two neural networks that compete with each other in a zero-sum game where one's gain is the other's loss. The GAN is trained on a set of data and uses this information to generate new data with the same characteristics as the training set. For instance, a GAN trained on photos can produce new photos that appear realistic to human eyes. Although originally created for unsupervised learning, GANs have also shown success in semi-supervised, fully supervised, and reinforcement learning. The core concept of a GAN is the indirect training through a discriminator network that assesses the realism of the generated data. This setup allows the generator to learn in an unsupervised manner, not by minimizing the difference to a specific image, but by fooling the discriminator.

Dataset can be downloaded from: https://www.kaggle.com/jessicali9530/celeba-dataset

Sample Input Image:
![image](https://user-images.githubusercontent.com/38975177/217202336-bcc4412b-b794-4002-bf4e-a816570c2f05.png)


Output Sample Image:

![image](https://user-images.githubusercontent.com/38975177/217202616-a28ebd05-76c3-440c-9d2b-da6b56f1aeb9.png)

![image](https://user-images.githubusercontent.com/38975177/217202829-e243943e-3978-4b4c-a03a-da7b7210549c.png)

![image](https://user-images.githubusercontent.com/38975177/217202871-f9fb5466-7cae-4a96-b9e2-aeadd1570829.png)

## Structure

A GAN consists of two components: a generator and a discriminator. The generator is trained to produce believable data, which then serves as negative examples for the discriminator. The discriminator, on the other hand, is trained to differentiate between the generator's fake data and real data, penalizing the generator for generating unrealistic results. At the start of training, the generator produces clearly fake data, allowing the discriminator to quickly identify it as such.

## Discriminator

The discriminator has two sources of training data: real data instances (e.g. actual photos of people) that serve as positive examples, and fake data instances generated by the generator, which serve as negative examples. The discriminator is connected to two loss functions, one for the generator and one for itself. During discriminator training, it only uses its own loss function, classifying both real and fake data and penalizing itself for misidentifying them. The weights of the discriminator are updated through backpropagation from the discriminator loss. During generator training, the generator loss is used.

## Generator

The generator component of a GAN learns to produce fake data by taking into account the feedback from the discriminator, specifically by learning to produce data that the discriminator classifies as real. The training process for the generator is more closely intertwined with the discriminator than the training process for the discriminator. This involves the following steps: providing the generator with random input, using the generator network to convert this input into a data instance, using the discriminator network to classify the generated data, receiving the discriminator output, and using the generator loss to penalize the generator for not successfully tricking the discriminator.

![image](https://user-images.githubusercontent.com/38975177/217203818-7f77ad97-ce84-4b30-b432-0c3106f18402.png)


## Training

The generator and discriminator have different training processes and in order to train the GAN as a whole, we alternate between training the two components. This is done by repeating the following steps: training the discriminator for one or more epochs, followed by training the generator for one or more epochs. The generator is kept constant during the discriminator training phase as the discriminator needs to learn how to identify the generator's flaws, which is a different task for a well-trained generator than for an untrained generator producing random output. Similarly, the discriminator is kept constant during the generator training phase so that the generator is not trying to fool a moving target and can converge.
